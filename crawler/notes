==================================================
;SECTIONS

sections
todo
crawl heuristics
incremental save
bugs
refactor
uncategorized


==================================================
;TODO


blacklisting heat logic
clean up

==================================================
;CRAWL HEURISTICS

ad stink

can the script request an ad?
  any js
  request
  third-party url in script as a string constant

boredom
  same parent for too long
  domain based?

heat
	tracked for each parent and domain
	each loop step is counted


==================================================
;INCREMENTAL SAVE


sites & blacklist after each entry

queue can be regenerated

sites as ordered dict?
  no queue
  just find the oldest uncrawled site

just key lists in class

but blacklist is a set

blacklist as ordered dict
blacklist as list
blackdict

domain list

==================================================
;BUGS

duplicate crawling

pages like this - https://www.rpmfind.net/linux/RPM/CentOS.html 
	too many links
	links to non-html stuff?
    not on that particlular page
requesting just content info
  deeper dive into http

https://episcopalchurch.org/files/ellibrodeoracioncomun_0.pdf
	doesn't detect wrong filetype

sites with iframes?
	https://www.cartoongamez.com/game10.html

sneaky bug
  site instead of link
  
==================================================
;REFACTOR

pages are dataclasses


"fetch" downloads the page and sets its status
  a method of Page

"crawl":
  - checks blacklist
  - tries to fetch the page
  - looks for links
  - adds an inlink to each linked page:
      linked pages are added to the repo if they don't exist
      ...
    ^^ meh, a lot of messing around with stuff that doesn't come to mind when you think "crawl"
      but that's simple stuff
    
but fetching and crawling separately means passing outlinks
          

page never exists on its own
  page repo?


site doesn't push itself
  push_site does

domain dict
  heat
  blocked?

domain as
  dict?
  dataclass?
  class?

==================================================
;TESTING

crawler-test.com

==================================================
;UNCATEGORIZED


scrapy?

deduplication
  hash the html
    hash as the key?
  strip stuff after '#' in urls

"known" storing urls or sites?
  does it matter?
  it's just a pointer

hasad server

rank heuristics and crawl heuristics


"Site" as class
  storing parents and children?

heuristics updated while site in queue?

dead domains 



storing offline html?

async?

ordered dict and infinite iteration
