==================================================
;SECTIONS

sections
todo
hackier
writeup
architecture
filter
crawler
low level crwaling stuff
high level crwaling stuff
scrapy
common crawl
search
uncategorized

==================================================
;TODO

==================================================
;HACKIER

glue up as much as possible

crawler with scrapy
search with
  whoosh
    old
  lunrjs
  elasticlunrjs
    they have nicely styled homepage
    they tell me not to do it
      do they really?
  sphinx
  manticore
    

==================================================
;WRITEUP


Crocus - it's basically adblock^2

search for the actual information


a search engine for the part of web that's actually interesting

an attempt at making the internet awesome again
search engine for the non-garbage internet

PoC || GTFO?

50000 crocuses
	weirdness without hype
		not that hype is necessarily bad


sites to be read and navigated


spartan nojs

psychological problem


what is the web for
  getting information
  does JS improve you getting the information



amish?

most js is garbage or provides very little value

no ads before dom load + no deferred js
	checking for deferred js
		puppeteer?


is pagerank pwnable?

;business stuff

mismatch between value to the end user and the value to the contractor
  speak normally, please

newsletters to ask for email
SEO spam


google papers


computers and programmers need food
we sell the part of it that really scales
  ml models, indexing,

how google really makes money search
  the "ad" checkbox?
    does my adblock block it?
      doesn't seem like so

gugl
	freeloaders
implicit stochastic pimping

one of these ad boards

==================================================
;ARCHITECTURE

interface between systems
  filter writes to the domains file
  crawler writes to crawled domains file
  cron job sends the diff of both files to the crawler to be crawled
  same can be done for indexer ingestion

or maybe it can be done as a single pipe
  but it should be done in a way where I don't have to restart everything

maybe write both to a pipe and a file
when restarting, the file can be yeeted to a pipe
^^ do it with files for now

making use of cc metrics


the problem with resource allocation


the systems
  filter
  crawler
  indexer, search

named pipes
  are they good?

homepage-first ad-free heuristic
  but do it for hosts, if they are what I think they are
    check it out!
filter home pages with puppeteer
scrape clean websites with scrapy

filter-crawler interface
  generator that waits for a signal?

==================================================
;FILTER

the abstraction I'm looking for
  a promise queue that automagically gets filled up to n promises using a generator
    async pool

concurrent tabs
  resolve an array of promises concurrently
  is there something like that for generators?

consent-o-matic

installing extensions for puppeteer

making it distributed

take the commoncrawl host site links
  [url]
  why not use the alexa 1 million?
    is it up to date
    how is it made

how about crawling the hosts?
  are github.io pages hosts?
    prolly
      check commoncrawl

setting $DISPLAY

weird ID bug
  apparently not opening each url in new page helps
gr

false negatives
  paypal.com
  "
    facebook.com
    instagram.com
  "
    using just one page helps
      one blocker per page?
        it uses an external file, is it ok to instantiate it?
        one way to find out!
        also sniff when it I/Os
  en.wikipedia.org
    disabling and re-enabling blocker after each page helped
  "
    archive.org TODO
    github.com
  "
    one blocker per ?page sequence?
    
not closing pages at all

not awaiting page close helps with the weird error

install the same adblock, use the same list
use probe
check if preceding site makes a difference

false positives
  bbc.co.uk
    there's a delayed redirect

ProtocolError: Network.setCacheDisabled timed out. Increase the 'protocolTimeout' setting in launch/connect calls for a higher timeout if needed.

==================================================
;CRAWLER


testing
  crawler-test.com
  
setting request priority

prioritizing homepages when crawling?
  scrapy depth priority


why doesn't it quit
  is that while loop necessary?

making scrapy operate a UNIX named pipe
  there's a pipe
  another process puts stuffs there

the crawler shouldn't quit when no crawls are scheduled
derviative requests should proceed normally

crawler vs spider in scrapy

multiple spiders?

==================================================
;LOW LEVEL CRWALING STUFF

problems
  high load time pages
    examples:
      high latency pages
      low bandwidth pages
      huge pages
    can I emulate them?
  how not to get blocked?
^^ scrapy has download timeouts
  read up on how they are implemented

redirects

==================================================
;HIGH LEVEL CRWALING STUFF

correct header - what about xml, xhmtl & stuff?
uncrawlables can be identified by extension as well

mime type and detected mime type
mime type in response headers?

==================================================
;SCRAPY

"you don't need scrapy for JS, because it can do requests concurrently already"
  seems really fishy, but why?
    stuff like number of concurrent requests

vk detecting that I'm not using chrome
  supposedly html headers other than UA leak information about the browser
    copied headers, still doesn't work

making it run in the bg

blacklist with scrapy?
  just a normal set, like in the old crawler
  there's some blacklist stuff for distributed scrapy on the web

IgnoreRequest
StopDownload
^^what did I want that for?

RTFM (all of it!)

about 30000 pages over night
  it's much worse tham my previous crawler
^^?clobbered? by slow requests?


why it didn't have the header problem in the "parse" method?
  what header problem?
    i was trying to yeet requests to documents with wrong file types

can the DNS server be throttling me?
twistedIO threadpool
what is twisted anyway?
how many concurrent requests is enough
is there a way to measure my current total download bandwidth
what's the exact message?

telnet console
  does it work on the other computer?

manually setting request priority 
does scrapy schedule non-start requests when start_requests didn't finish?
  how to test that?

==================================================
;COMMON CRAWL

common crawl
  first, I just want page urls and their links
  or maybe just download subsets of data,
  yeet stuff with scripts, download more, etc
    tried, bad idea

  is offset in bytes or lines?
    what offset?

==================================================
;SEARCH

small-scale search index

that quickwit stuff might be too sophisticated
maybe try elasticlunrjs or smth

quickwit

==================================================
;UNCATEGORIZED

reorganize the fustercluck

\    _    /
|\__/ \__/|
| _|+_+|_ |
|/ \_ _/ \|
/   | |   \
   
