
read the old code

just crawlin for links first?
  that is, not persistantly storing pages

making it run in the bg

problems
  high load time pages
    examples:
      high latency pages
      low bandwidth pages
      huge pages
    can I emulate them?
  how not to get blocked?
^^ scrapy has download timeouts
  read up on how they are implemented

figure out scrapy
  it probably solves more problems than I know I have
    yeah, stuff like redirects

crawl only through html
  

blacklist with scrapy?
  just a normal set, like in the old crawler
  there's some blacklist stuff for distributed scrapy on the web

IgnoreRequest
StopDownload
^^what did I want that for?

RTFM (all of it!)

it kinda feels slower
  randomness probably

about 30000 pages over night
  it's much worse tham my previous crawler

correct header - what about xml, xhmtl & stuff?
uncrawlables can be identified by extension as well

mime type and detected mime type
mime type in response headers?

quickwit

why it didn't have the header problem in the "parse" method?

what's a decorator?

common crawl
  first, I just want page urls and their links
  or maybe just download subsets of data, yeet stuff with scripts, download more, etc
  is offset in bytes or lines?

webgraph trick
  shallow crawl of each domain homepage for ads
    noscript first
  filter stuff from cc index
  
alexa 1 million?

can the DNS server be throttling me?
twistedIO threadpool
what is twisted anyway?
how many concurrent requests is enough
is there a way to measure my current total download bandwidth
what's the exact message?

vk detecting that I'm not using chrome
  supposedly html headers other than UA leak information about the browser

how about crawling the hosts?
  are github.io pages hosts?
    prolly
      check commoncrawl


making it sane on 2 computers
  edit on the server?

adprobe server
making it distributed

telnet console
  does it work on the other computer?


js thingy

take the commoncrawl top site list
  why not use the alexa ranking?
    is it up to date
    how is it made

homepage-first ad-free heuristic
  but do it for hosts, if they are what I think they are
    check it out!
filter home pages with puppeteer
scrape clean websites with scrapy

ideally I'll do a little bit of each all at once

how to make the subsystems not get in the way of each other?
  apparently you can read an open file
    when I try to do it in python the file is empty
    can I check if the file is used?
    do I really need it for searching?
      well, I need some kind of a pipe...
      can't you just wait till the file is free?

can you read a file that's being written?
  what happens if you try?
    does it break?
    does it wait?

named pipes
  are they good?

manually setting request priority in scrapy
does scrapy schedule non-start requests when start_requests didn't finish?
  how to test that?

filter - crawler interface


generator that waits for a signal?


prioritizing homepages when crawling?
  scrapy depth priority

the systems
  host filter
  crawler
  indexer, search
    hopefully quickwit does that

interface between systems


the problem with resource allocation


consent-o-matic

installing extensions for puppeteer

concurrent tabs
  resolve an array of promises concurrently
  is there something like that for generators?


reorganize the fustercluck

how google really makes money search
  the "ad" checkbox?
    does my adblock block it?
      doesn't seem like so

small-scale search index
craw

crawler-test.com

inspect other sites with js detection misses
  or not

many cool sites use benign scripts

todo:
  figure out how to work on the remote computer sanely
  check out the hosts rank from CC
  write a filter
  quickwit

keep the data separately

mop up this file?
