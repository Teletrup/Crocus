read the old code

just crawlin for links first?

making it run in the bg

problems
  high load time pages
    examples:
      high latency pages
      low bandwidth pages
      huge pages
    can I emulate them?
  how not to get blocked?

figure out scrapy
it probably solves more problems than I know I have

understanding http
  is requesting headers separately much slower?

does latency affect bandwidth for TCP?
  should depend on ?frame? size
    the acks have to travel...

monitor traffic
  is it tru that outgoing requests come on different ports?

how is incomming traffic from 2 different sources at once, possible?
  it isn't at once
  it's queued, by router probably
    rly?

crawl only through html
  

fakesrv
  te

blacklist with scrapy?

IgnoreRequest
StopDownload

RTFM (all of it!)

it kinda feels slower
  randomness probably

about 30000 pages over night
  it's much worse tham my previous crawler

quickwit

correct header - what about xml, xhmtl & stuff?

why it didn't have the header problem in the "parse" method?

what's a decorator?

common crawl
  first, I just want page urls and their links
  or maybe just download subsets of data, yeet stuff with scripts, download more, etc
  is offset in bytes or lines?

webgraph trick
  shallow crawl of each domain homepage for ads
    noscript first
  filter stuff from cc index
  
alexa 1 million?

can the DNS server be throttling me?
twistedIO threadpool
what is twisted anyway?
how many concurrent requests is enough
is there a way to measure my current total download bandwidth
what's the exact message?

vk detecting that I'm not using chrome
  supposedly html headers other than UA leak information about the browser

how about crawling the hosts?
  are github.io pages hosts?
    prolly
      check commoncrawl

homepage-first ad-free heuristic

making it sane on 2 computers
  edit on the server?

adprobe server
making it distributed

telnet console
  does it work on the other computer?


js thingy

take the commoncrawl top site list
  why not use the alexa ranking?
    is it up to date
    how is it made
filter home pages with puppeteer
scrape clean websites with scrapy

ideally I'll do a little bit of each all at once

how to make the subsystems not get in the way of each other?
  apparently you can read an open file
  can you read a file that's being written?
    what happens if you try?
      does it break?
      does it wait?

the problem with resource allocation


consent-o-matic

installing extensions for puppeteer

concurrent tabs
  resolve an array of promises concurrently
  is there something like that for generators?


reorganize the fustercluck

how google really makes money search
  the "ad" checkbox?
    does my adblock block it?
      doesn't seem like so



